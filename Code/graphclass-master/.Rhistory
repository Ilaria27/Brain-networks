main="Posterior of N")
abline(v=mle.N,col="red")
#  compute the posterior mean!!
# If I have now the power of computing the posterior of N in the rescaled and vectorized
# style, I just have to take the weighted sum of the support points times the corresponding
# posterior mass evaluations at the same support points.
sum((1:N.max)*posterior.N.rescaled.vec(N=1:N.max,observed_counts=25,p=p0))
#  compute the posterior mean!!
# If I have now the power of computing the posterior of N in the rescaled and vectorized
# style, I just have to take the weighted sum of the support points times the corresponding
# posterior mass evaluations at the same support points.
sum((1:N.max)*posterior.N.rescaled.vec(N=1:N.max,observed_counts=5,p=p0))
# This is the weighted average of the supports, with the weights being provided by the
# posterior mass evaluation.
which.max(posterior.N.rescaled.vec(N=1:N.max,observed_counts=5,p=p0))
# 16th position.
# If that has been evaluated in (1:Nmax) grid of values, and I take the 16th element:
(1:N.max)[which.max(posterior.N.rescaled.vec(N=1:N.max,observed_counts=5,p=p0))]
#  compute the posterior median!! (of a discrete distribution)
# The median is between the mean and the mode. We compute the median looking at which values
# in the support points 1:N.max, are such that the cumulative sum of the posterior is greater
# than 0.5
min((1:N.max)[cumsum(posterior.N.rescaled.vec(N=1:N.max,observed_counts=5,p=p0))>=0.5])
# can we draw a sample from the posterior?
# We can use the sample() function. We have to give the support points as a first argument (x),
# then the probability masses for each support point (prob), and I am taking the sample with
# replacement, so that I have i.i.d. simulations.
post_N_sample <- sample(x=1:N.max,
prob=posterior.N.rescaled.vec(N=1:N.max,observed_counts=s_obs,p=p0),
replace = TRUE,size=1000000)
length(post_N_sample)
# If I take that large sample, and compute the median of this empirical distribution:
median(post_N_sample)
# can we draw a sample from the posterior?
# We can use the sample() function. We have to give the support points as a first argument (x),
# then the probability masses for each support point (prob), and I am taking the sample with
# replacement, so that I have i.i.d. simulations. The posterior distribution is of the paramater.
post_N_sample <- sample(x=1:N.max,
prob=posterior.N.rescaled.vec(N=1:N.max,observed_counts=5,p=p0),
replace = TRUE,size=1000000)
# If I take that large sample, and compute the median of this empirical distribution:
median(post_N_sample)
mean(post_N_sample)
# Swich again to 1 observation:
set.seed(123)
y <- s_obs <- rbinom(n=1,size=20,prob=0.3) # = 5
# BUT WHY THE FOLLOWING RESULT?
sum(posterior.N.vec(1:N.max,observed_counts=s_obs,p=p0))
N_support <- 1:N.max
posterior.N.vec(1:100,observed_counts=5,p = p0)
sum(posterior.N.vec(1:N.max,observed_counts=5,p = p0)) # = 3.333333 - The sum of this vector of
# I should plot this in the scale which is porvided without the proportionality normilizing
# constant.
plot(x=1:N.max,y=posterior.N.vec(1:N.max,observed_counts=5,p=p0),type="h",
xlab="N",ylab=expression(pi*group("(","",".")*N*group("|",s,".")*group(".","",")")),
main="Posterior of N")
abline(v=mle.N,col="red")
# BUT WHY THE FOLLOWING RESULT?
sum(posterior.N.vec(1:N.max,observed_counts=s_obs,p=p0))
1:N.max
plot(x=1:N.max,y=posterior.N.vec(1:N.max,observed_counts=s_obs,p=p0),type="h",
xlab="N",ylab=expression(pi*group("(","",".")*N*group("|",s,".")*group(".","",")")),main="Posterior of N")
abline(v=mle.N,col="red")
posterior.N.vec(1:100,observed_counts=s_obs,p = p0)
# I should plot this in the scale which is porvided without the proportionality normilizing
# constant.
plot(x=1:N.max,y=posterior.N.vec(1:N.max,observed_counts=s_obs,p=p0),type="h",
xlab="N",ylab=expression(pi*group("(","",".")*N*group("|",s,".")*group(".","",")")),
main="Posterior of N")
abline(v=mle.N,col="red")
sum(posterior.N.vec(1:N.max,observed_counts=s_obs,p=p0)/
sum(posterior.N.vec(1:N.max,observed_counts=s_obs,p=p0)))
posterior.N.rescaled <- function(N,observed_counts,p){
exp(log.posterior.N.vec(N,observed_counts,p))/
sum(exp(log.posterior.N.vec(1:N.max,observed_counts,p)))
}
posterior.N.rescaled.vec <- Vectorize(posterior.N.rescaled,"N")
args(posterior.N.rescaled.vec)
posterior.N.rescaled.vec(N=1:200,observed_counts=5,p=p0)
sum(posterior.N.rescaled.vec(N=1:N.max,observed_counts=5,p=p0))
plot(x=1:N.max,y=posterior.N.rescaled.vec(1:N.max,observed_counts=5,p=p0),type="h",
xlab="N",ylab=expression(pi*group("(","",".")*N*group("|",s,".")*group(".","",")")),
main="Posterior of N")
abline(v=mle.N,col="red")
# If I have now the power of computing the posterior of N in the rescaled and vectorized
# style, I just have to take the weighted sum of the support points times the corresponding
# posterior mass evaluations at the same support points.
sum((1:N.max)*posterior.N.rescaled.vec(N=1:N.max,observed_counts=s_obs,p=p0)) # = 19
# This is the weighted average of the supports, with the weights being provided by the
# posterior mass evaluation.
# We see in which value the posterior evaluationn reach the maximum:
which.max(posterior.N.rescaled.vec(N=1:N.max,observed_counts=s_obs,p=p0)) # = 16 - It is the
# If that has been evaluated in (1:Nmax) grid of values, and I take the 16th element:
(1:N.max)[which.max(posterior.N.rescaled.vec(N=1:N.max,observed_counts=s_obs,p=p0))] # it is 16
#  compute the posterior median!! (of a discrete distribution)
# The median is between the mean and the mode. We compute the median looking at which values
# in the support points 1:N.max, are such that the cumulative sum of the posterior is greater
# than 0.5.
min((1:N.max)[cumsum(posterior.N.rescaled.vec(N=1:N.max,observed_counts=s_obs,p=p0))>=0.5]) # = 18
# can we draw a sample from the posterior?
# We can use the sample() function. We have to give the support points as a first argument (x),
# then the probability masses for each support point (prob), and I am taking the sample with
# replacement, so that I have i.i.d. simulations. The posterior distribution is of the paramater.
post_N_sample <- sample(x=1:N.max,
prob=posterior.N.rescaled.vec(N=1:N.max,observed_counts=s_obs,p=p0),
replace = TRUE,size=1000000)
# If I take that large sample, and compute the median of this empirical distribution:
median(post_N_sample) # = 18 - corresponds to the one I computed before.
mean(post_N_sample) # = 19.00866 - Not exactly the same, but very close.
y = 3
likelihood = function(theta) {
dbinom(x = y, prob = theta, size = 20)
}
curve(likelihood, from = 0, to = 1)
N_start = c(50, 100, 200, 500, 1000, 2000)
TP = c(0.87, 0.83, 0.71, 0.63, 0.53, 0.34)
plot(N_start, TP)
?plot
plot(N_start, TP, type = 'l')
points(TP, add = T)
points(N_start, TP, add = T)
plot(N_start, TP, type = 'l')
points(N_start, TP, add = T)
plot(N_start, TP, type = 'l', lwd = 2)
points(N_start, TP, add = T)
?points
plot(N_start, TP, type = 'l', lwd = 2, col = 'red')
points(N_start, TP, add = T, pch = 18)
plot(N_start, TP, type = 'l', lwd = 2, col = 'blue')
points(N_start, TP, add = T, pch = 18)
plot(N_start, TP, type = 'l', lwd = 2, col = 'green')
points(N_start, TP, add = T, pch = 18)
## DOMANDA 6
y_obs <- c(6, 3, 5, 4, 1, 4, 2)
n = length(y_obs)
alpha_prior = (4.28^2)/9
beta_prior = alpha_prior/4.28
alpha_post = alpha_prior + sum(y_obs)
beta_post = beta_prior + n
pgamma(1, shape = alpha_prior, rate = beta_prior)
N = 20
server_list = 1:20
server_list
server_ID = 1:20
server_ID
queue_length = rep(0, 20)
queue_length
server_ID_out = []
server_ID_out = c()
for (server in server_ID) {
if (queue_length[server] > 0) {
s = sample(server_ID, 1)
list.remove(server_ID, s)
else
o = sample(server_ID_out, 1)
}
}
for (server in server_ID) {
if (queue_length[server] > 0) {
s = sample(server_ID, 1)
list.remove(server_ID, s)}
else {
o = sample(server_ID_out, 1)
}
}
server_ID_out = 1:N
server_ID_out = c()
library(hash)
library('hash')
if (!require("devtools")) install.packages("devtools")
devtools::install_github("mkuhn/dict")
install.packages("devtools")
devtools::install_github("mkuhn/dict")
library(dict)
install.packages("devtools")
library(devtools)
queue_length = vector(mode = 'list', length = N)
names(queue_length) = c('s_1', 's_2', 's_3', 's_4', 's_5', 's_6', 's_7', 's_8', 's_9', 's_10',
's_11', 's_12', 's_13', 's_14', 's_15', 's_16', 's_17', 's_18', 's_19',
's_20')
for (i in length(queue_length)) {
queue_length[[i]] = 0
}
queue_length[[1]]
queue_length
for (i in 1:length(queue_length)) {
queue_length[[i]] = 0
}
queue_length
queue_length[[1]]
queue_length[1]
n = 100000
r = 0
d = 3
server_ID[1]
list.remove(server_ID, 2)
server_ID = [1:N]
list.remove(server_ID, 2)
server_ID = list(1:N)
server_ID
server_ID = list([1:N])
server_ID = list(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20)
server_ID
list.remove(server_ID, 2)
server_ID[- 2]
server_ID
server_ID = server_ID[- 2]
server_ID
server_ID = list(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20)
server_ID_out = list()
server_ID[21] = 0
server_ID = list(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20)
sample(queue_length, d)
min(sample(queue_length, d))
min(c(sample(queue_length, d)))
unlist(sample(queue_length, d), use.names=FALSE)
min(unlist(sample(queue_length, d), use.names=FALSE))
N = 20
n = 100000
r = 0
d = 3
server_ID = list(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20)
server_ID_out = list()
queue_length = vector(mode = 'list', length = N)
names(queue_length) = c('s_1', 's_2', 's_3', 's_4', 's_5', 's_6', 's_7', 's_8', 's_9', 's_10',
's_11', 's_12', 's_13', 's_14', 's_15', 's_16', 's_17', 's_18', 's_19',
's_20')
for (i in 1:length(queue_length)) {
queue_length[[i]] = 0
}
queue_length[1]
msg = 0
for (t in 1:n) {
if (length(server_ID > 0)) {
s = sample(server_ID, 1)
server_ID = server_ID[- s]
queue_length[[s]] = queue_length[[s]] + 1
}
else {
o = sample(server_ID_out, 1)
queue_length[[o]] = queue_length[[o]] + 1
}
for (server in 1:N) {
if ((queue_length[[server]] < r) & (server !in server_ID)) {
server_ID[server] = queue_length[[server]]
msg = msg + 1
}
}
if (length(server_ID) == 0) {
sample_d = sample(queue_length, d)
r = min(unlist(sample(queue_length, d), use.names=FALSE))
}
}
for (t in 1:n) {
if (length(server_ID > 0)) {
s = sample(server_ID, 1)
server_ID = server_ID[- s]
queue_length[[s]] = queue_length[[s]] + 1
}
else {
o = sample(server_ID_out, 1)
queue_length[[o]] = queue_length[[o]] + 1
}
for (server in 1:N) {
if ((queue_length[[server]] < r) & (server !(in server_ID))) {
server_ID[server] = queue_length[[server]]
msg = msg + 1
}
}
if (length(server_ID) == 0) {
sample_d = sample(queue_length, d)
r = min(unlist(sample(queue_length, d), use.names=FALSE))
}
}
for (t in 1:n) {
if (length(server_ID > 0)) {
s = sample(server_ID, 1)
server_ID = server_ID[- s]
queue_length[[s]] = queue_length[[s]] + 1
}
else {
o = sample(server_ID_out, 1)
queue_length[[o]] = queue_length[[o]] + 1
}
for (server in 1:N) {
if ((queue_length[[server]] < r) & (server !(%in% server_ID))) {
server_ID[server] = queue_length[[server]]
msg = msg + 1
}
}
if (length(server_ID) == 0) {
sample_d = sample(queue_length, d)
r = min(unlist(sample(queue_length, d), use.names=FALSE))
}
}
for (t in 1:n) {
if (length(server_ID > 0)) {
s = sample(server_ID, 1)
server_ID = server_ID[- s]
queue_length[[s]] = queue_length[[s]] + 1
}
else {
o = sample(server_ID_out, 1)
queue_length[[o]] = queue_length[[o]] + 1
}
for (server in 1:N) {
if ((queue_length[[server]] < r) & (server !(in) server_ID)) {
server_ID[server] = queue_length[[server]]
msg = msg + 1
}
}
if (length(server_ID) == 0) {
sample_d = sample(queue_length, d)
r = min(unlist(sample(queue_length, d), use.names=FALSE))
}
}
for (t in 1:n) {
if (length(server_ID > 0)) {
s = sample(server_ID, 1)
server_ID = server_ID[- s]
queue_length[[s]] = queue_length[[s]] + 1
}
else {
o = sample(server_ID_out, 1)
queue_length[[o]] = queue_length[[o]] + 1
}
for (server in 1:N) {
if ((queue_length[[server]] < r) & !(server in server_ID)) {
server_ID[server] = queue_length[[server]]
msg = msg + 1
}
}
if (length(server_ID) == 0) {
sample_d = sample(queue_length, d)
r = min(unlist(sample(queue_length, d), use.names=FALSE))
}
}
server_ID.names
server_ID.names()
server_ID
names(server_ID)
for (t in 1:n) {
if (length(server_ID > 0)) {
s = sample(server_ID, 1)
server_ID = server_ID[- s]
queue_length[[s]] = queue_length[[s]] + 1
}
else {
o = sample(server_ID_out, 1)
queue_length[[o]] = queue_length[[o]] + 1
}
for (server in 1:N) {
if ((queue_length[[server]] < r) & !(server in unlist(server_ID))) {
server_ID[server] = queue_length[[server]]
msg = msg + 1
}
}
if (length(server_ID) == 0) {
sample_d = sample(queue_length, d)
r = min(unlist(sample(queue_length, d), use.names=FALSE))
}
}
server_ID[- 2]
server_ID[[- 2]]
2 == server_ID[- 2]
N_start = c(50, 100, 200, 500, 1000, 1600, 2000)
TP_f = c(0.87, 0.84, 0.71, 0.63, 0.53, 0.41, 0.32)
plot(N_start, TP_f, type = 'l', lwd = 2, col = 'green')
points(N_start, TP_f, add = T, pch = 18)
U = runif(10000)
X = -log(1-U) # random variable X which is a transformation of U
hist(X, prob=T)
curve(dexp(x), 0, 12, add=T, col='red', lwd=2)
curve(dbeta(x,2,4),col="red",lwd=2,ylim=c(0,4))
curve(dbeta(x,1,1),col="blue",lwd=2,add=TRUE,ylim=c(0,0))
k=3
curve(k*dbeta(x,1,1),col="blue",lwd=2,add=TRUE)
x_grid=seq(0,1,length=100000)
ef=function(x){
dbeta(x,2,4)
}
k_star <- max(ef(x_grid))
k=k_star # approximate (slightly underestimating the exact maximum)
curve(dunif(x)*k_star,0,1,xlab="x",ylab=expression(f[X](x)),ylim=c(0,4),lwd=2)
curve(dbeta(x,2,4),add=TRUE,col="red",lwd=2)
text(0.8,3.5,labels=expression(k~f[U](x)))
text(0.8,0.7,labels=expression(f[X](x)),col="red")
legend(x="topleft",lty=1,lwd=2.4,col=c("red","black"),legend=c("target density","bounding function"))
title(main="A/R")
ef=function(x){
dbeta(x,2,4)
}
q=function(x){
dunif(x)
}
k=3
n_sim_aux=10000
Y=rep(NA,n_sim_aux)
E=rep(NA,n_sim_aux)
for(i in 1:n_sim_aux){
Y[i]=runif(1)
E[i]=rbinom(1,size=1,prob=ef(Y[i])/(k*q(Y[i])))
}
set.seed(123)
n_sim_aux=10
Y=rep(NA,n_sim_aux)
E=rep(NA,n_sim_aux)
for(i in 1:n_sim_aux){
Y[i]=runif(1) # I simuluate this from the auxiliary function
E[i]=rbinom(1,size=1,prob=ef(Y[i])/(k*q(Y[i]))) # the probability I put inside the binom
# function is the probability of ACCEPTANCE
}
head(cbind(Y,E))
set.seed(123)
n_sim_aux=20
Y=rep(NA,n_sim_aux)
E=rep(NA,n_sim_aux)
for(i in 1:n_sim_aux){
Y[i]=runif(1) # I simuluate this from the auxiliary function
E[i]=rbinom(1,size=1,prob=ef(Y[i])/(k*q(Y[i]))) # the probability I put inside the binom
# function is the probability of ACCEPTANCE
}
head(cbind(Y,E))
cbind(Y,E)
# with n_sim_aux = 10 I accept only 1 value.
# I accept the simulations of Y for which E = 1
X <- Y # this is wrong, because I am taking even the non accepted values
X[E==0] <- NA
head(cbind(Y,E,X))
t(head(cbind(Y,E,X)))
head(cbind(Y,E))
cbind(Y,E)[1:20,]
# We remove NA and keep only the non missing values
# corresponding to the accepted Y[i]'s
X=Y[E==1]
length(X)
hist(X,prob=TRUE)
curve(ef(x),add=TRUE,col="red",lwd=3)
prop.table(table(E))
1/k
sum(E)
length(X)
mean(E)
require(R2jags)
install.packages("R2jags")
require(R2jags)
require(mcmcse)
install.packages("mcmcse")
require(mcmcse)
require(bayesplot)
install.packages("bayesplot")
require(bayesplot)
require(bayesplot)
install.packages("bayesplot")
require(bayesplot)
require(TeachingDemos)
install.packages("bayesplot")
if (!require("devtools")) {
install.packages("devtools")
}
devtools::install_github("stan-dev/bayesplot")
install.packages("devtools")
devtools::install_github("stan-dev/bayesplot")
library(devtools)
install_github("jesusdaniel/graphclass")
install_github("jesusdaniel/graphclass")
install_github("jesusdaniel/graphclass")
install_github("jesusdaniel/graphclass")
library(devtools)
install_github("jesusdaniel/graphclass")
install_github("jesusdaniel/graphclass")
library(devtools)
install_github("jesusdaniel/graphclass")
install_github("jesusdaniel/graphclass")
install_github("jesusdaniel/graphclass")
# install_github("jesusdaniel/graphclass")
# install.packages("penalizedSVM", repos='http://cran.us.r-project.org')
library(penalizedSVM)
library(graphclass)
source("Classifiers/Cross-validation-function.R")
data(COBRE.data)
plot_adjmatrix(COBRE.data$X.cobre[1,])
X <- COBRE.data$X.cobre
Y <- COBRE.data$Y.cobre
Xnorm <- apply(X, 2, function(v) (v-mean(v))/sd(v))
rho_seq <- c(10^(seq(2.5, -2, length.out = 31)))
grid = data.frame(rbind(1e-4, rho_seq, 1e-5))
parameters_grid <- lapply(grid, function(x) x)
NODES <- (1+sqrt(1+8*ncol(Xnorm)))/2
D <- construct_D(NODES)
fold_index <- (1:length(Y) %% 5) + 1
gclist <- list()
for(fold in 1:5) {
foldout <- which(fold_index == fold)
gclist[[fold]] <- graphclass(X = X[-foldout,], Y = Y[-foldout],
Xtest = X[foldout,], Ytest = Y[foldout],
type = "intersection",
lambda = 1e-4, rho = 1, gamma = 1e-5,
D = D)
}
Ypreds = lapply(gclist, function(gc) gc$Ypred)
Ypreds[[1]]
Ytest = Y[foldout]
acc = sum(array(Ypreds[[5]]) == Ytest)/length(Ytest)
save(acc, file="accuracy.txt")
save(acc, file="accuracy.Rdata")
setwd("C:/Users/Ilaria T/Desktop/Sapienza/Tesi Brain Network/Code/graphclass-master")
save(acc, file="accuracy.txt")
save(acc, file="accuracy.Rdata")
write.table(acc, file="accuracy.txt")
write.table(acc, file="accuracy.txt", row.names = F)
write.table(acc, file="accuracy.txt", row.names = F, col.names = F)
