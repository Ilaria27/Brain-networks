points(TP, add = T)
points(N_start, TP, add = T)
plot(N_start, TP, type = 'l')
points(N_start, TP, add = T)
plot(N_start, TP, type = 'l', lwd = 2)
points(N_start, TP, add = T)
?points
plot(N_start, TP, type = 'l', lwd = 2, col = 'red')
points(N_start, TP, add = T, pch = 18)
plot(N_start, TP, type = 'l', lwd = 2, col = 'blue')
points(N_start, TP, add = T, pch = 18)
plot(N_start, TP, type = 'l', lwd = 2, col = 'green')
points(N_start, TP, add = T, pch = 18)
## DOMANDA 6
y_obs <- c(6, 3, 5, 4, 1, 4, 2)
n = length(y_obs)
alpha_prior = (4.28^2)/9
beta_prior = alpha_prior/4.28
alpha_post = alpha_prior + sum(y_obs)
beta_post = beta_prior + n
pgamma(1, shape = alpha_prior, rate = beta_prior)
N = 20
server_list = 1:20
server_list
server_ID = 1:20
server_ID
queue_length = rep(0, 20)
queue_length
server_ID_out = []
server_ID_out = c()
for (server in server_ID) {
if (queue_length[server] > 0) {
s = sample(server_ID, 1)
list.remove(server_ID, s)
else
o = sample(server_ID_out, 1)
}
}
for (server in server_ID) {
if (queue_length[server] > 0) {
s = sample(server_ID, 1)
list.remove(server_ID, s)}
else {
o = sample(server_ID_out, 1)
}
}
server_ID_out = 1:N
server_ID_out = c()
library(hash)
library('hash')
if (!require("devtools")) install.packages("devtools")
devtools::install_github("mkuhn/dict")
install.packages("devtools")
devtools::install_github("mkuhn/dict")
library(dict)
install.packages("devtools")
library(devtools)
queue_length = vector(mode = 'list', length = N)
names(queue_length) = c('s_1', 's_2', 's_3', 's_4', 's_5', 's_6', 's_7', 's_8', 's_9', 's_10',
's_11', 's_12', 's_13', 's_14', 's_15', 's_16', 's_17', 's_18', 's_19',
's_20')
for (i in length(queue_length)) {
queue_length[[i]] = 0
}
queue_length[[1]]
queue_length
for (i in 1:length(queue_length)) {
queue_length[[i]] = 0
}
queue_length
queue_length[[1]]
queue_length[1]
n = 100000
r = 0
d = 3
server_ID[1]
list.remove(server_ID, 2)
server_ID = [1:N]
list.remove(server_ID, 2)
server_ID = list(1:N)
server_ID
server_ID = list([1:N])
server_ID = list(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20)
server_ID
list.remove(server_ID, 2)
server_ID[- 2]
server_ID
server_ID = server_ID[- 2]
server_ID
server_ID = list(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20)
server_ID_out = list()
server_ID[21] = 0
server_ID = list(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20)
sample(queue_length, d)
min(sample(queue_length, d))
min(c(sample(queue_length, d)))
unlist(sample(queue_length, d), use.names=FALSE)
min(unlist(sample(queue_length, d), use.names=FALSE))
N = 20
n = 100000
r = 0
d = 3
server_ID = list(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20)
server_ID_out = list()
queue_length = vector(mode = 'list', length = N)
names(queue_length) = c('s_1', 's_2', 's_3', 's_4', 's_5', 's_6', 's_7', 's_8', 's_9', 's_10',
's_11', 's_12', 's_13', 's_14', 's_15', 's_16', 's_17', 's_18', 's_19',
's_20')
for (i in 1:length(queue_length)) {
queue_length[[i]] = 0
}
queue_length[1]
msg = 0
for (t in 1:n) {
if (length(server_ID > 0)) {
s = sample(server_ID, 1)
server_ID = server_ID[- s]
queue_length[[s]] = queue_length[[s]] + 1
}
else {
o = sample(server_ID_out, 1)
queue_length[[o]] = queue_length[[o]] + 1
}
for (server in 1:N) {
if ((queue_length[[server]] < r) & (server !in server_ID)) {
server_ID[server] = queue_length[[server]]
msg = msg + 1
}
}
if (length(server_ID) == 0) {
sample_d = sample(queue_length, d)
r = min(unlist(sample(queue_length, d), use.names=FALSE))
}
}
for (t in 1:n) {
if (length(server_ID > 0)) {
s = sample(server_ID, 1)
server_ID = server_ID[- s]
queue_length[[s]] = queue_length[[s]] + 1
}
else {
o = sample(server_ID_out, 1)
queue_length[[o]] = queue_length[[o]] + 1
}
for (server in 1:N) {
if ((queue_length[[server]] < r) & (server !(in server_ID))) {
server_ID[server] = queue_length[[server]]
msg = msg + 1
}
}
if (length(server_ID) == 0) {
sample_d = sample(queue_length, d)
r = min(unlist(sample(queue_length, d), use.names=FALSE))
}
}
for (t in 1:n) {
if (length(server_ID > 0)) {
s = sample(server_ID, 1)
server_ID = server_ID[- s]
queue_length[[s]] = queue_length[[s]] + 1
}
else {
o = sample(server_ID_out, 1)
queue_length[[o]] = queue_length[[o]] + 1
}
for (server in 1:N) {
if ((queue_length[[server]] < r) & (server !(%in% server_ID))) {
server_ID[server] = queue_length[[server]]
msg = msg + 1
}
}
if (length(server_ID) == 0) {
sample_d = sample(queue_length, d)
r = min(unlist(sample(queue_length, d), use.names=FALSE))
}
}
for (t in 1:n) {
if (length(server_ID > 0)) {
s = sample(server_ID, 1)
server_ID = server_ID[- s]
queue_length[[s]] = queue_length[[s]] + 1
}
else {
o = sample(server_ID_out, 1)
queue_length[[o]] = queue_length[[o]] + 1
}
for (server in 1:N) {
if ((queue_length[[server]] < r) & (server !(in) server_ID)) {
server_ID[server] = queue_length[[server]]
msg = msg + 1
}
}
if (length(server_ID) == 0) {
sample_d = sample(queue_length, d)
r = min(unlist(sample(queue_length, d), use.names=FALSE))
}
}
for (t in 1:n) {
if (length(server_ID > 0)) {
s = sample(server_ID, 1)
server_ID = server_ID[- s]
queue_length[[s]] = queue_length[[s]] + 1
}
else {
o = sample(server_ID_out, 1)
queue_length[[o]] = queue_length[[o]] + 1
}
for (server in 1:N) {
if ((queue_length[[server]] < r) & !(server in server_ID)) {
server_ID[server] = queue_length[[server]]
msg = msg + 1
}
}
if (length(server_ID) == 0) {
sample_d = sample(queue_length, d)
r = min(unlist(sample(queue_length, d), use.names=FALSE))
}
}
server_ID.names
server_ID.names()
server_ID
names(server_ID)
for (t in 1:n) {
if (length(server_ID > 0)) {
s = sample(server_ID, 1)
server_ID = server_ID[- s]
queue_length[[s]] = queue_length[[s]] + 1
}
else {
o = sample(server_ID_out, 1)
queue_length[[o]] = queue_length[[o]] + 1
}
for (server in 1:N) {
if ((queue_length[[server]] < r) & !(server in unlist(server_ID))) {
server_ID[server] = queue_length[[server]]
msg = msg + 1
}
}
if (length(server_ID) == 0) {
sample_d = sample(queue_length, d)
r = min(unlist(sample(queue_length, d), use.names=FALSE))
}
}
server_ID[- 2]
server_ID[[- 2]]
2 == server_ID[- 2]
N_start = c(50, 100, 200, 500, 1000, 1600, 2000)
TP_f = c(0.87, 0.84, 0.71, 0.63, 0.53, 0.41, 0.32)
plot(N_start, TP_f, type = 'l', lwd = 2, col = 'green')
points(N_start, TP_f, add = T, pch = 18)
U = runif(10000)
X = -log(1-U) # random variable X which is a transformation of U
hist(X, prob=T)
curve(dexp(x), 0, 12, add=T, col='red', lwd=2)
curve(dbeta(x,2,4),col="red",lwd=2,ylim=c(0,4))
curve(dbeta(x,1,1),col="blue",lwd=2,add=TRUE,ylim=c(0,0))
k=3
curve(k*dbeta(x,1,1),col="blue",lwd=2,add=TRUE)
x_grid=seq(0,1,length=100000)
ef=function(x){
dbeta(x,2,4)
}
k_star <- max(ef(x_grid))
k=k_star # approximate (slightly underestimating the exact maximum)
curve(dunif(x)*k_star,0,1,xlab="x",ylab=expression(f[X](x)),ylim=c(0,4),lwd=2)
curve(dbeta(x,2,4),add=TRUE,col="red",lwd=2)
text(0.8,3.5,labels=expression(k~f[U](x)))
text(0.8,0.7,labels=expression(f[X](x)),col="red")
legend(x="topleft",lty=1,lwd=2.4,col=c("red","black"),legend=c("target density","bounding function"))
title(main="A/R")
ef=function(x){
dbeta(x,2,4)
}
q=function(x){
dunif(x)
}
k=3
n_sim_aux=10000
Y=rep(NA,n_sim_aux)
E=rep(NA,n_sim_aux)
for(i in 1:n_sim_aux){
Y[i]=runif(1)
E[i]=rbinom(1,size=1,prob=ef(Y[i])/(k*q(Y[i])))
}
set.seed(123)
n_sim_aux=10
Y=rep(NA,n_sim_aux)
E=rep(NA,n_sim_aux)
for(i in 1:n_sim_aux){
Y[i]=runif(1) # I simuluate this from the auxiliary function
E[i]=rbinom(1,size=1,prob=ef(Y[i])/(k*q(Y[i]))) # the probability I put inside the binom
# function is the probability of ACCEPTANCE
}
head(cbind(Y,E))
set.seed(123)
n_sim_aux=20
Y=rep(NA,n_sim_aux)
E=rep(NA,n_sim_aux)
for(i in 1:n_sim_aux){
Y[i]=runif(1) # I simuluate this from the auxiliary function
E[i]=rbinom(1,size=1,prob=ef(Y[i])/(k*q(Y[i]))) # the probability I put inside the binom
# function is the probability of ACCEPTANCE
}
head(cbind(Y,E))
cbind(Y,E)
# with n_sim_aux = 10 I accept only 1 value.
# I accept the simulations of Y for which E = 1
X <- Y # this is wrong, because I am taking even the non accepted values
X[E==0] <- NA
head(cbind(Y,E,X))
t(head(cbind(Y,E,X)))
head(cbind(Y,E))
cbind(Y,E)[1:20,]
# We remove NA and keep only the non missing values
# corresponding to the accepted Y[i]'s
X=Y[E==1]
length(X)
hist(X,prob=TRUE)
curve(ef(x),add=TRUE,col="red",lwd=3)
prop.table(table(E))
1/k
sum(E)
length(X)
mean(E)
require(R2jags)
install.packages("R2jags")
require(R2jags)
require(mcmcse)
install.packages("mcmcse")
require(mcmcse)
require(bayesplot)
install.packages("bayesplot")
require(bayesplot)
require(bayesplot)
install.packages("bayesplot")
require(bayesplot)
require(TeachingDemos)
install.packages("bayesplot")
if (!require("devtools")) {
install.packages("devtools")
}
devtools::install_github("stan-dev/bayesplot")
install.packages("devtools")
devtools::install_github("stan-dev/bayesplot")
library(devtools)
install_github("jesusdaniel/graphclass")
install_github("jesusdaniel/graphclass")
install_github("jesusdaniel/graphclass")
install_github("jesusdaniel/graphclass")
library(devtools)
install_github("jesusdaniel/graphclass")
install_github("jesusdaniel/graphclass")
library(devtools)
install_github("jesusdaniel/graphclass")
install_github("jesusdaniel/graphclass")
install_github("jesusdaniel/graphclass")
library(devtools)
install_github("jesusdaniel/graphclass")
source("Cross-validation-function.R")
setwd("C:/Users/Ilaria T/Desktop/Sapienza/Tesi Brain Network/Code/graphclass-master/Classifiers")
source("Cross-validation-function.R")
source("train-test-functions-cv.R")
#---- Load network dataset
#   X matrix with rows for individuals and columns for edges,
#     taking the upper triangular part of the adjacency matrix)
#   Y class labels
#load("/data/jarroyor/data/COBRE/Data/COBRE_Xranks")
#load("../GRAPHCLASS_2/Data/COBRE_Xranks")
#load("../GRAPHCLASS_4/COBRE_Folds_1jun_1_10.RData")
#Standardize edges
Xnorm <- apply(Xranks, 2, function(v) (v-mean(v))/sd(v))
#---- Load network dataset
#   X matrix with rows for individuals and columns for edges,
#     taking the upper triangular part of the adjacency matrix)
#   Y class labels
load("/data/jarroyor/data/COBRE/Data/COBRE_Xranks")
#---- Load network dataset
#   X matrix with rows for individuals and columns for edges,
#     taking the upper triangular part of the adjacency matrix)
#   Y class labels
load("../data/jarroyor/data/COBRE/Data/COBRE_Xranks")
#---- Load network dataset
#   X matrix with rows for individuals and columns for edges,
#     taking the upper triangular part of the adjacency matrix)
#   Y class labels
#load("../data/jarroyor/data/COBRE/Data/COBRE_Xranks")
load("../data/COBRE/Data/COBRE_Xranks")
### SVM L1
parameters_grid <- lapply(10^seq(-1,4,length.out = 31), function(x) x)
svmL1_cv <- cross_validation_function(X = Xnorm, Y = factor(Y), parameters_grid = parameters_grid,
methodname = "penalizedSVM-L1",
train_classifier = train_svml1, test_classifier = test_svml1,
folds = 1, fold_list = folds_list,
parallel = F, num_clusters = 10, windows = T,
nested_cv = F, save_files = F, filename = "", sparsity_results=T)
load("C:/Users/Ilaria T/Desktop/Sapienza/Tesi Brain Network/Code/graphclass-master/data/COBRE.data.rda")
View(COBRE.data)
Xnorm <- apply(Xranks, 2, function(v) (v-mean(v))/sd(v))
load("C:/Users/Ilaria T/Desktop/Sapienza/Tesi Brain Network/Code/graphclass-master/data/power.parcellation.rda")
View(power.parcellation)
#---- Load network dataset
#   X matrix with rows for individuals and columns for edges,
#     taking the upper triangular part of the adjacency matrix)
#   Y class labels
#load("../data/jarroyor/data/COBRE/Data/COBRE_Xranks")
#load("../GRAPHCLASS_2/Data/COBRE_Xranks")
#load("../GRAPHCLASS_4/COBRE_Folds_1jun_1_10.RData")
#Standardize edges
Xranks = COBRE.data[["X.cobre"]]
View(Xranks)
Xnorm <- apply(Xranks, 2, function(v) (v-mean(v))/sd(v))
View(Xnorm)
svmL1_cv <- cross_validation_function(X = Xnorm, Y = factor(Y), parameters_grid = parameters_grid,
methodname = "penalizedSVM-L1",
train_classifier = train_svml1, test_classifier = test_svml1,
folds = 1, fold_list = folds_list,
parallel = F, num_clusters = 10, windows = T,
nested_cv = F, save_files = F, filename = "", sparsity_results=T)
View(cross_validation_function)
naivebayes_cv <- cross_validation_function(X = Xnorm, Y = factor(Y), parameters_grid = parameters_grid,
methodname = "naivebayes",
train_classifier = train_naivebayes, test_classifier = test_naivebayes,
folds = 10, fold_list = NULL,
parallel = T, num_clusters = 4, windows = T,
nested_cv = F, save_files = F, filename = "", sparsity_results=T)
### SVM L2
parameters_grid <- list(0.01,0.1, 1, 10, 100, 1000)
svme1071_cv <- cross_validation_function(X = Xnorm, Y = factor(Y), parameters_grid = parameters_grid,
methodname = "svm-e1071",algorithm_parameters = "linear",
train_classifier = train_svml1, test_classifier = test_svml1,
folds = 10, fold_list = folds_list,
parallel = T, num_clusters = 10, windows = T,
nested_cv = F, save_files = F, filename = "", sparsity_results=F)
### Random forest
parameters_grid <- list(c(100, 500), c(200, 500), c(300, 500),
c(100, 1000), c(200, 1000), c(300, 1000),
c(100, 2000), c(200, 2000), c(300, 2000))
rf1_cv <- cross_validation_function(X = Xnorm, Y = factor(Y), parameters_grid = parameters_grid,
methodname = "randomforest1",
train_classifier = train_randomforest, test_classifier = test_randomforest,
folds = 10, fold_list = folds_list,
parallel = T, num_clusters = 10, windows = T,
nested_cv = F, save_files = F, filename = "", sparsity_results=F)
# With nested CV
source("CV/Cross-validation-function.R")
Xnorm <- apply(Xranks, 2, function(v) (v-mean(v))/sd(v))
### GC
rho_seq <- c(10^(seq(2.5, -2, length.out = 31)))
grid = data.frame(rbind(1e-4, rho_seq, 1e-5))
parameters_grid <- lapply(grid, function(x) x)
NODES <- (1+sqrt(1+8*ncol(Xnorm)))/2
D <- construct_D(NODES)
install.packages("penalizedSVM", repos='http://cran.us.r-project.org')
library(penalizedSVM)
D <- construct_D(NODES)
library(devtools)
library(graphclass)
data(COBRE.data)
plot_adjmatrix(COBRE.data$X.cobre[1,])
X <- COBRE.data$X.cobre
Y <- COBRE.data$Y.cobre
Xnorm <- apply(X, 2, function(v) (v-mean(v))/sd(v))
install.packages("penalizedSVM", repos='http://cran.us.r-project.org')
library(penalizedSVM)
source("CV/Cross-validation-function.R")
source("Cross-validation-function.R")
source("train-test-functions-cv.R")
gc1_cv <- cross_validation_function(X = Xnorm, Y = factor(Y),
parameters_grid = parameters_grid,
methodname = "gc1",
train_classifier = train_graphclass,
test_classifier = test_graphclass,
folds = 10, fold_list = folds_list,
algorithm_parameters = list(D = D),
parallel = T, num_clusters = 10, windows = T,
nested_cv = F, save_files = F, filename = "",
sparsity_results=T)
gc1_cv <- cross_validation_function(X = Xnorm, Y = factor(Y),
parameters_grid = parameters_grid,
methodname = "gc1",
train_classifier = train_graphclass,
test_classifier = test_graphclass,
folds = 10, algorithm_parameters = list(D = D),
parallel = T, num_clusters = 10, windows = T,
nested_cv = F, save_files = F, filename = "",
sparsity_results=T)
gc1_cv <- cross_validation_function(X = Xnorm, Y = factor(Y),
parameters_grid = NULL,
methodname = "gc1",
train_classifier = train_graphclass,
test_classifier = test_graphclass,
folds = 10, algorithm_parameters = list(D = D),
parallel = T, num_clusters = 10, windows = T,
nested_cv = F, save_files = F, filename = "",
sparsity_results=T)
gc1_cv <- cross_validation_function(X = Xnorm, Y = factor(Y),
parameters_grid = NULL,
methodname = "gc1",
train_classifier = train_graphclass(X, Y),
test_classifier = test_graphclass(train_classifier, X, Y),
folds = 10, algorithm_parameters = list(D = D),
parallel = T, num_clusters = 10, windows = T,
nested_cv = F, save_files = F, filename = "",
sparsity_results=T)
gc1_cv <- cross_validation_function(X = Xnorm, Y = factor(Y),
methodname = "gc1",
train_classifier = train_graphclass(X, Y),
test_classifier = test_graphclass(train_classifier, X, Y),
folds = 10, algorithm_parameters = list(D = D),
parallel = T, num_clusters = 10, windows = T,
nested_cv = F, save_files = F, filename = "",
sparsity_results=T)
